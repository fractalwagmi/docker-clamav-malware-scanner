const {logger} = require('./logger.js');
const unzip = require('unzip-stream');
const clamd = require('clamdjs');
const {CLAMD_MAX_FILE_SIZE} = require('./constants.js');

const SIZE_LIMIT_ERROR = 'INSTREAM size limit exceeded';
const FALLBACK_RESOLVE_TIMEOUT = 10 * 1000; // 10 seconds
const CHUNK_SCAN_OVERLAP_FACTOR = 0.1;

/**
 * @param {*} scanner
 * @param {*} gcsFile
 * @param {number} timeoutPerFile
 * @param {number} chunkSize
 * @return {Promise<Array<*>>} collection of clamd results
 */
function scanGcsZipFile(scanner, gcsFile, timeoutPerFile, chunkSize) {
  const scanPromises = [];

  let fileCount = 0;
  let scannedCount = 0;

  return new Promise((resolve, reject) => {
    const readstream = gcsFile.createReadStream();

    // First, try to scan zip archive fully and scan each individual file in
    // full with clamd. If that fails, then break larger files up into smaller
    // chunks to scan to avoid clam's hard 4GB limit.
    // eslint-disable-next-line new-cap
    readstream.pipe(unzip.Parse()).on('entry', async (entry) => {
      if (entry.type === 'File') {
        logger.info(`
          Scanning ${entry.path} with size ${entry.size},
          file ${++fileCount}/N
        `);

        // scan with chunks if size is defined and over 4GB
        let promise;
        if (entry.size && entry.size > CLAMD_MAX_FILE_SIZE) {
          promise = scanChunked(
              scanner, gcsFile, entry.path, timeoutPerFile, chunkSize);
        } else {
          promise = scanFully(scanner, entry, timeoutPerFile)
              .catch((err) => {
                // dispose of the original stream since we no longer need it
                entry.autodrain();
                if (err.message.includes(SIZE_LIMIT_ERROR)) {
                  logger.warn(`
                      Full scan failed for ${entry.path} due to size limit
                      reached. Falling back to chunked scan.
                  `);
                  return scanChunked(
                      scanner, gcsFile, entry.path, timeoutPerFile, chunkSize);
                } else {
                  throw err;
                }
              });
        }
        const promiseWithFilename = promise.then((result) => {
          logger.info(
              `Finished scanning ${entry.path} with result ${result}`,
          );
          logger.info(`Scanned ${++scannedCount} of ${fileCount}`);
          return {result, fileName: entry.path};
        });
        scanPromises.push(promiseWithFilename);
        promiseWithFilename.then(() => {
          if (readstream.closed) {
            return;
          }

          // This is hacky but sometimes the 'finish' or 'close' event on the
          // outer stream is never emitted so we need to manually check if all
          // per-file scans have completed. Wait a few seconds and if the number
          // of total and scanned files is unchanged, then we can assume all
          // files have started their scan and we can resolve when all scans
          // are completed. It's bad practice but OK to call `resolve` in
          // multiple places but only the first call will succeed.
          if (scannedCount === fileCount) {
            logger.info(`
              Scanned count equals file count. Checking if all files have been
              added to the scan queue.
            `);
            const expectedTotalCount = fileCount;
            setTimeout(() => {
              if (fileCount === expectedTotalCount &&
                scannedCount === fileCount) {
                logger.info(`
                  Finished processing ${scanPromises.length} entries.
                `);
                Promise.all(scanPromises)
                    .then((results) => {
                      logger.info(
                          `Finished scanning ${results.length} results`);
                      resolve(results);
                    })
                    .catch((err) => reject(err));
              }
            }, FALLBACK_RESOLVE_TIMEOUT);
          }
        });
      } else {
        // ignore directories
        entry.autodrain();
      }
    }).on('close', () => {
      logger.info(`
        Finished processing ${scanPromises.length} entries...awaiting results.
      `);
      Promise.all(scanPromises)
          .then((results) => {
            logger.info(`Finished processing ${results.length} results`);
            resolve(results);
          })
          .catch((err) => reject(err));
    }).on('error', (err) => {
      logger.error(`Error scanning zip file: ${err}`);
      reject(err);
    });
  });
}

/**
 * @param {*} scanner
 * @param {*} readstream
 * @param {*} timeoutPerFile
 * @return {Promise<string>}
 */
function scanFully(scanner, readstream, timeoutPerFile) {
  return scanner.scanStream(readstream, timeoutPerFile);
}

/**
 * @param {*} scanner
 * @param {*} gcsFile
 * @param {number} filePath
 * @param {number} timeoutPerFile
 * @param {number} chunkSize
 * @return {Promise<string>} clamd result
 */
function scanChunked(scanner, gcsFile, filePath, timeoutPerFile, chunkSize) {
  const rs = gcsFile.createReadStream();

  const chunks = [];
  let bufferSize = 0;
  const promises = [];
  let chunkCount = 0;

  const processChunks = () => {
    try {
      const buffer = Buffer.concat(chunks);

      // Remove (1 - overlap factor) * buffer size in bytes so we can scan
      // overlapping ranges of data if malware signatures cross the chunk
      // boundary.
      while (bufferSize > chunkSize * CHUNK_SCAN_OVERLAP_FACTOR) {
        bufferSize -= chunks.shift().length;
      }

      const promise = scanner.scanBuffer(buffer, timeoutPerFile);
      promises.push(promise);
      promise.then((result) =>
        logger.info(
            `Finished scanning chunk ${++chunkCount} with result ${result}`,
        ));
    } catch (err) {
      logger.error(`Error scanning chunk: ${err}`);
      rs.destroy(err);
    }
  };

  return new Promise((resolve, reject) => {
    // eslint-disable-next-line new-cap
    rs.pipe(unzip.Parse()).on('entry', (entry) => {
      // Scan .zip again but look for specific file entry.
      if (entry.path === filePath) {
        // entry is a readstream
        entry.on('data', async (chunk) => {
          chunks.push(chunk);
          bufferSize += chunk.length;

          // drain the buffer + scan if we reached the max chunk size
          if (bufferSize >= chunkSize) {
            processChunks();
          }
        }).on('end', () => {
          // process any remaining data
          logger.info(
              `Processing remaining buffer of size ${bufferSize} for
              ${filePath}`,
          );
          processChunks();
        });
      } else {
        entry.autodrain();
      }
    }).on('close', () => {
      Promise.all(promises).then((results) => {
        logger.info(
            `Finished scanning ${promises.length} chunks for ${filePath}`,
        );

        // check if any results are not clean
        const infectedResult = results.find((result) =>
          !clamd.isCleanReply(result));
        if (infectedResult) {
          resolve(infectedResult);
        } else {
          resolve(results[0]);
        }
      }).catch((err) => reject(err));
    }).on('error', (err) => {
      logger.error(`Error scanning zip file with chunks: ${err}`);
      reject(err);
    });
  });
}

module.exports = scanGcsZipFile;
